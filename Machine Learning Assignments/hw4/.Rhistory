# Assess the performance on the testing set
test_results_titanic <- augment(final_logreg_fit_titanic, new_data = test_data_titanic)
# Calculate ROC AUC on the testing set
test_roc_auc_titanic <- test_results_titanic %>%
roc_auc(truth = survived, .pred_Yes)
# Print the ROC AUC on the testing set
print(test_roc_auc_titanic)
# Finalize the logistic regression workflow
final_logreg_wf_titanic <- finalize_workflow(logreg_wf_titanic)
# Extract the best parameters
best_params <- logreg_results_titanic %>%
select_best(metric = "roc_auc")
# Finalize the workflow with the best parameters
final_logreg_wf_titanic <- finalize_workflow(logreg_wf_titanic, best_params)
# Fit the finalized workflow to the entire training set
final_logreg_fit_titanic <- fit(final_logreg_wf_titanic,
data = train_data_titanic)
# Get the predictions on the testing set
test_results_titanic <- augment(final_logreg_fit_titanic,
new_data = test_data_titanic)
# Reorder the factor levels so that "Yes" is the reference level
test_results_titanic$survived <- relevel(test_results_titanic$survived,
ref = "Yes")
# Calculate ROC AUC on the testing set
test_roc_auc_titanic <- test_results_titanic %>%
roc_auc(truth = survived, .pred_Yes)
# Print the ROC AUC on the testing set
print(test_roc_auc_titanic)
## From the result in Q11, we have the AUC value for Logistic regression model is 0.8525593.
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
warning = FALSE)
# Load the required packages first.
library(tidymodels)
library(ISLR)
library(ISLR2)
library(tidyverse)
library(glmnet)
library(modeldata)
library(ggthemes)
library(janitor)
library(naniar)
library(corrplot)
library(themis)
library(MASS)
library(discrim)
library(dplyr)
library(rsample)
library(recipes)
tidymodels_prefer()
# Create the recipe with manual interactions using step_mutate()
abalone_recipe <- recipe(age ~ ., data = train_data) %>%
update_role(rings, new_role = "ID variable") %>%
step_dummy(type, one_hot = TRUE) %>%
step_mutate(interaction_1 = type_M * shucked_weight,
interaction_2 = longest_shell * diameter,
interaction_3 = shucked_weight * shell_weight) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors())
# Print the recipe to check it
print(abalone_recipe)
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
warning = FALSE)
# Load the required packages first.
library(tidymodels)
library(ISLR)
library(ISLR2)
library(tidyverse)
library(glmnet)
library(modeldata)
library(ggthemes)
library(janitor)
library(naniar)
library(corrplot)
library(themis)
library(MASS)
library(discrim)
library(dplyr)
library(rsample)
library(recipes)
tidymodels_prefer()
# Set a seed for reproducibility
set.seed(123)
# Read the dataset
abalone_data <- read_csv("abalone.csv")
# Calculate the age (which is calculated as the number of rings + 1.5)
abalone_data <- abalone_data %>%
# Then create a new variable for 'age'
mutate(age = rings + 1.5)
# Perform stratified sampling to split the data into training and testing sets
data_split <- initial_split(abalone_data, prop = 0.8, strata = "age")
train_data <- training(data_split)
test_data <- testing(data_split)
# View the first few rows of both sets to confirm the split, and check 'age'
head(train_data)
head(test_data)
# Create 5 folds from the training set
folds <- vfold_cv(train_data, v = 5, strata = "age")
# Print the folds to check them
folds
# Create the recipe with manual interactions using step_mutate()
abalone_recipe <- recipe(age ~ ., data = train_data) %>%
update_role(rings, new_role = "ID variable") %>%
step_dummy(type, one_hot = TRUE) %>%
step_mutate(interaction_1 = type_M * shucked_weight,
interaction_2 = longest_shell * diameter,
interaction_3 = shucked_weight * shell_weight) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors())
# Print the recipe to check it
print(abalone_recipe)
# Create the recipe with manual interactions using step_mutate()
abalone_recipe <- recipe(age ~ ., data = train_data) %>%
update_role(rings, new_role = "ID variable") %>%
step_dummy(type, one_hot = TRUE) %>%
step_mutate(interaction_1 = type_M * shucked_weight,
interaction_2 = longest_shell * diameter,
interaction_3 = shucked_weight * shell_weight) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors())
# Print the recipe to check it
abalone_recipe
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
warning = FALSE)
# Load the required packages first.
library(tidymodels)
library(ISLR)
library(ISLR2)
library(tidyverse)
library(glmnet)
library(modeldata)
library(ggthemes)
library(janitor)
library(naniar)
library(corrplot)
library(themis)
library(MASS)
library(discrim)
library(dplyr)
library(rsample)
library(recipes)
tidymodels_prefer()
# Set a seed for reproducibility
set.seed(123)
# Read the dataset
abalone_data <- read_csv("abalone.csv")
# Calculate the age (which is calculated as the number of rings + 1.5)
abalone_data <- abalone_data %>%
# Then create a new variable for 'age'
mutate(age = rings + 1.5)
# Perform stratified sampling to split the data into training and testing sets
data_split <- initial_split(abalone_data, prop = 0.8, strata = "age")
train_data <- training(data_split)
test_data <- testing(data_split)
# View the first few rows of both sets to confirm the split, and check 'age'
head(train_data)
head(test_data)
# Create 5 folds from the training set
folds <- vfold_cv(train_data, v = 5, strata = "age")
# Print the folds to check them
folds
# Create the recipe with manual interactions using step_mutate()
abalone_recipe <- recipe(age ~ ., data = train_data) %>%
update_role(rings, new_role = "ID variable") %>%
step_dummy(type, one_hot = TRUE) %>%
step_mutate(interaction_1 = type_M * shucked_weight,
interaction_2 = longest_shell * diameter,
interaction_3 = shucked_weight * shell_weight) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors())
# Print the recipe to check it
abalone_recipe
## K-fold cross-validation is a resampling technique used to assess a model's
#performance when the true error is unknown. In k-fold cross-validation, the
#original training dataset is randomly partitioned into k equally sized
#subsamples, in other words, folds.
## Here are some advantages of using k-fold cross-validation:
# 1.Bias-Variance Tradeoff: It gives a more "unbiased" estimate of the model's
#performance. Training and testing on the same dataset is likely to lead to
#overfitting, where the model learns the noise in the data rather than the actual
#relationship between variables.
# 2.Better Generalization: By training the model on different subsets of the
#data, we can ensure that the model generalizes well to new data.
# 3.Resource Efficiency: It allows for efficient use of data as each observation
#is used for both training and validation, especially important if the available
#dataset is small.
## "Holdout method". In this approach, the model is trained on one subset and
#validated on a separate subset that was not used during training. The holdout
#method is simpler but less robust than k-fold cross-validation because the
#evaluation may depend heavily on which observations are included in the training
#and test sets.
# k-NN Model Specification
knn_spec <- nearest_neighbor(neighbors = tune(), engine = "kknn") %>%
set_mode("regression")
# k-NN Workflow
knn_wf <- workflow() %>%
add_model(knn_spec) %>%
add_recipe(abalone_recipe)
# k-NN Grid
knn_grid <- grid_regular(
neighbors(range = c(1, 10)),
levels = 10
)
# Linear Regression Model Specification
lm_spec <- linear_reg() %>%
set_engine("lm") %>%
set_mode("regression")
# Linear Regression Workflow
lm_wf <- workflow() %>%
add_model(lm_spec) %>%
add_recipe(abalone_recipe)
# Elastic Net Model Specification
enet_spec <- linear_reg(
penalty = tune(),
mixture = tune()
) %>%
set_engine("glmnet") %>%
set_mode("regression")
# Elastic Net Workflow
enet_wf <- workflow() %>%
add_model(enet_spec) %>%
add_recipe(abalone_recipe)
# Elastic Net Grid
enet_grid <- grid_regular(
penalty(),
mixture(range = c(0, 1)),
levels = 10
)
# Check both grid
print(knn_grid)
print(enet_grid)
# Number of folds
num_folds <- 5
# Number of parameter combinations for k-NN
num_knn_params <- nrow(knn_grid)
# Number of parameter combinations for Elastic Net
num_enet_params <- nrow(enet_grid)
# Linear Regression has only 1 model
num_lm_params <- 1
# Total number of models for each fold
total_models_each_fold <- num_knn_params + num_lm_params + num_enet_params
# Total number of models across all folds
total_models_all_folds <- num_folds * total_models_each_fold
# Print the total number of models across all folds
print(paste("Total number of models across all folds: ", total_models_all_folds))
# Fit k-NN model using tune_grid()
knn_results <- tune_grid(
knn_wf,
resamples = folds,
grid = knn_grid
)
# Fit Linear Regression model using fit_resamples()
lm_results <- fit_resamples(
lm_wf,
resamples = folds
)
# Fit Elastic Net model using tune_grid()
enet_results <- tune_grid(
enet_wf,
resamples = folds,
grid = enet_grid
)
# Check the results with detailed output
collect_metrics(knn_results)
collect_metrics(lm_results)
collect_metrics(enet_results)
# Check for notes or warnings
show_notes(knn_results)
show_notes(lm_results)
show_notes(enet_results)
# Collect metrics for k-NN
knn_metrics <- collect_metrics(knn_results)
# Collect metrics for Linear Regression
lm_metrics <- collect_metrics(lm_results)
# Collect metrics for Elastic Net
enet_metrics <- collect_metrics(enet_results)
# For k-NN: filter for RMSE and sort the models by RMSE for each fold
best_knn <- knn_metrics %>%
filter(.metric == "rmse") %>%
arrange(mean) %>%
slice_head(n = 1)
# For Elastic Net: filter for RMSE and sort the models by RMSE for each fold
best_enet <- enet_metrics %>%
filter(.metric == "rmse") %>%
arrange(mean) %>%
slice_head(n = 1)
# For Linear Regression: Since there's only one model, take the RMSE as is
best_lm <- lm_metrics %>%
filter(.metric == "rmse") %>%
summarise(mean_rmse = mean(mean), std_err_rmse = mean(std_err))
# Print best models for each fold for k-NN and Elastic Net
print(best_knn)
print(best_enet)
# Print RMSE for Linear Regression
print(best_lm)
## Based on the output above, the best k-NN model has 10 neighbors and an RMSE of
#approximately 2.300986 with a standard error of 0.05772904. The best Elastic Net #model has a penalty of 1e-10 and a mixture of 0.1111111, which has an RMSE of
#2.182027 with a standard error of 0.06192046. And the Linear Regression
#model has an RMSE of 2.182331 with a standard error of 0.06114094.
## Based on the RMSE values, both the Elastic Net and Linear Regression models
#have nearly the same performance, and they are slightly better than the best
#k-NN model. However, since Linear Regression performs almost as well as Elastic
#Net but is simpler, it may be considered the best model for this specific
#problem based on Occam's razor principle, which suggests that among models with
#similar performance, the simpler one is often preferable.
# Fit the workflow to the entire training set
final_lm_fit <- fit(lm_wf, data = train_data)
# Assess the performance on the testing set
test_results <- augment(final_lm_fit, new_data = test_data)
# Calculate RMSE on the testing set
test_rmse <- test_results %>%
metrics(truth = age, estimate = .pred) %>%
filter(.metric == "rmse")
# Print the RMSE on the testing set
print(test_rmse)
## From question 5, we have managed to obtain RMSE for linear regression model,
#which is 2.182331. And from the output above, we have the RMSE on the testing
#set is 2.186413. By observing two RMSE values, the RMSE for the testing set is
#very close to the average RMSE obtained from the cross-validation. This is a
#good sign, indicating that the model generalizes well to new, unseen data.
# Load the Titanic data
titanic_data <- read_csv("titanic.csv")
# Convert 'survived' and 'pclass' to factors
titanic_data$survived <- as.factor(titanic_data$survived)
titanic_data$pclass <- as.factor(titanic_data$pclass)
# Set a seed for reproducibility
set.seed(42)
# Perform stratified sampling to split the data into training and testing sets
data_split_titanic <- initial_split(titanic_data, prop = 0.7,
strata = "survived")
train_data_titanic <- training(data_split_titanic)
test_data_titanic <- testing(data_split_titanic)
# Create 5 folds from the training set
folds_titanic <- vfold_cv(train_data_titanic, v = 5, strata = "survived")
# Print the folds to check them
print(folds_titanic)
# Define the recipe for the Titanic dataset
recipe_titanic <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare,
data = train_data_titanic) %>%
# Upsample to balance the classes in 'survived'
step_upsample(survived, over_ratio = 1) %>%
# Impute missing values for 'age' using linear imputation
step_impute_linear(age, impute_with = imp_vars(sib_sp)) %>%
# Dummy encode categorical predictors
step_dummy(all_nominal_predictors()) %>%
# Include interaction terms
step_interact(~ starts_with("sex") : age + age:fare)
# Print and check the recipe
print(recipe_titanic)
# k-NN Model Specification for Titanic
knn_spec_titanic <- nearest_neighbor(neighbors = tune(),
engine = "kknn") %>%
set_mode("classification")
# k-NN Workflow for Titanic
knn_wf_titanic <- workflow() %>%
add_model(knn_spec_titanic) %>%
add_recipe(recipe_titanic)
# Logistic Regression Model Specification for Titanic
logreg_spec_titanic <- logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
# Logistic Regression Workflow for Titanic
logreg_wf_titanic <- workflow() %>%
add_model(logreg_spec_titanic) %>%
add_recipe(recipe_titanic)
# Elastic Net Logistic Regression Model Specification for Titanic
enet_spec_titanic <- logistic_reg(
penalty = tune(),
mixture = tune()
) %>%
set_engine("glmnet") %>%
set_mode("classification")
# Elastic Net Workflow for Titanic
enet_wf_titanic <- workflow() %>%
add_model(enet_spec_titanic) %>%
add_recipe(recipe_titanic)
# k-NN Grid for Titanic
knn_grid_titanic <- grid_regular(
neighbors(range = c(1, 10)),
levels = 10
)
# Elastic Net Grid for Titanic
enet_grid_titanic <- grid_regular(
penalty(),
mixture(range = c(0, 1)),
levels = 10
)
# Check both grids to ensure they are correctly set up
print(knn_grid_titanic)
print(enet_grid_titanic)
# Fit k-NN Model
knn_results_titanic <- tune_grid(
object = knn_wf_titanic,
resamples = folds_titanic,
grid = knn_grid_titanic
)
# Fit Logistic Regression Model
logreg_results_titanic <- tune_grid(
object = logreg_wf_titanic,
resamples = folds_titanic
)
# Fit Elastic Net Model
enet_results_titanic <- tune_grid(
object = enet_wf_titanic,
resamples = folds_titanic,
grid = enet_grid_titanic
)
# Check the results with detailed output
collect_metrics(knn_results_titanic)
collect_metrics(logreg_results_titanic)
collect_metrics(enet_results_titanic)
# Check for notes or warnings
show_notes(knn_results_titanic)
show_notes(logreg_results_titanic)
show_notes(enet_results_titanic)
# Collect metrics for k-NN
knn_metrics_titanic <- collect_metrics(knn_results_titanic)
# Collect metrics for Logistic Regression
logreg_metrics_titanic <- collect_metrics(logreg_results_titanic)
# Collect metrics for Elastic Net
enet_metrics_titanic <- collect_metrics(enet_results_titanic)
# For k-NN: filter for AUC and sort the models by AUC for each fold
best_knn_titanic <- knn_metrics_titanic %>%
filter(.metric == "roc_auc") %>%
arrange(mean) %>%
slice_head(n = 1)
# For Elastic Net: filter for AUC and sort the models by AUC for each fold
best_enet_titanic <- enet_metrics_titanic %>%
filter(.metric == "roc_auc") %>%
arrange(mean) %>%
slice_head(n = 1)
# For Logistic Regression: Since there's only one model, take the AUC as is
best_logreg_titanic <- logreg_metrics_titanic %>%
filter(.metric == "roc_auc") %>%
summarise(mean_auc = mean(mean), std_err_auc = mean(std_err))
# Print best models for each fold for k-NN and Elastic Net
print(best_knn_titanic)
print(best_enet_titanic)
# Print AUC for Logistic Regression
print(best_logreg_titanic)
## Based on the output above, we know that The k-NN model has an AUC of 0.7700835
#with a standard error of 0.02847453. The Elastic Net model has an AUC of 0.5
#with a standard error of 0, which suggests that it performed no better than
#random guessing. And the Logistic Regression model has an AUC of 0.8525593	with
#a standard error of 0.02295994.
## The model with the highest AUC is the Logistic Regression model, with an AUC
#of about 0.8526. A higher AUC indicates better performance, so in this case,
#with a relatively low standard error of approximately 0.023, the Logistic
#Regression model is the best-performing model among the three.
# Extract the best parameters
best_params <- logreg_results_titanic %>%
select_best(metric = "roc_auc")
# Finalize the workflow with the best parameters
final_logreg_wf_titanic <- finalize_workflow(logreg_wf_titanic, best_params)
# Fit the finalized workflow to the entire training set
final_logreg_fit_titanic <- fit(final_logreg_wf_titanic,
data = train_data_titanic)
# Get the predictions on the testing set
test_results_titanic <- augment(final_logreg_fit_titanic,
new_data = test_data_titanic)
# Reorder the factor levels so that "Yes" is the reference level
test_results_titanic$survived <- relevel(test_results_titanic$survived,
ref = "Yes")
# Calculate ROC AUC on the testing set
test_roc_auc_titanic <- test_results_titanic %>%
roc_auc(truth = survived, .pred_Yes)
# Print the ROC AUC on the testing set
print(test_roc_auc_titanic)
## From the result above, we have the AUC value is 0.8610768 on the testing set.
#And in Q11, we have the AUC value for Logistic regression model is 0.8525593.
#Both of these AUC values are close to each other and relatively high, indicating
#a good predictive performance.
# Define the recipe for the Titanic dataset
recipe_titanic <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare,
data = train_data_titanic) %>%
# Upsample to balance the classes in 'survived'
step_upsample(survived, over_ratio = 1) %>%
# Impute missing values for 'age' using linear imputation
step_impute_linear(age, impute_with = imp_vars(sib_sp)) %>%
# Dummy encode categorical predictors
step_dummy(all_nominal_predictors()) %>%
# Include interaction terms
step_interact(~ starts_with("sex") : age + age:fare)
# Print and check the recipe
recipe_titanic
# Define the recipe for the Titanic dataset
recipe_titanic <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare,
data = train_data_titanic) %>%
# Upsample to balance the classes in 'survived'
step_upsample(survived, over_ratio = 1) %>%
# Impute missing values for 'age' using linear imputation
step_impute_linear(age, impute_with = imp_vars(sib_sp)) %>%
# Dummy encode categorical predictors
step_dummy(all_nominal_predictors()) %>%
# Include interaction terms
step_interact(~ starts_with("sex") : age + age:fare)
# Print and check the recipe
recipe_titanic
# Create the recipe with manual interactions using step_mutate()
abalone_recipe <- recipe(age ~ ., data = train_data) %>%
update_role(rings, new_role = "ID variable") %>%
step_dummy(type, one_hot = TRUE) %>%
step_mutate(interaction_1 = type_M * shucked_weight,
interaction_2 = longest_shell * diameter,
interaction_3 = shucked_weight * shell_weight) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors())
# Print the recipe to check it
abalone_recipe
View(abalone_recipe)
print(abalone_recipe)
view(abalone_recipe)
print(abalone_recipe)
