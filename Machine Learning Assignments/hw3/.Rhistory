# Print and check the recipe
recipe_q4
# Specify a logistic regression model with the 'glm' engine
logistic_spec <- logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
# Create a workflow combining the recipe and model specification
workflow_q5 <- workflow() %>%
add_recipe(recipe_q4) %>%
add_model(logistic_spec)
# Store the results of fit()
fit_workflow_q5 <- fit(workflow_q5, data = train_data)
# Display the results to check the model fit
fit_workflow_q5
# Specify a Linear Discriminant Analysis model with the 'MASS' engine
lda_spec <- discrim_linear() %>%
set_engine("MASS") %>%
set_mode("classification")
# Create a new workflow combining recipe_q4 and the new LDA model specification.
workflow_q6 <- workflow() %>%
add_recipe(recipe_q4) %>%
add_model(lda_spec)
# Store the results of fit() in a new variable
fit_workflow_q6 <- fit(workflow_q6, data = train_data)
# Display the results to check the model fit
fit_workflow_q6
# Create a parsnip model specification for QDA
qda_spec <- discrim_quad() %>%
set_engine('MASS') %>%
set_mode('classification')
# Create a new workflow combining recipe_q4 and the new QDA model specification.
workflow_q7 <- workflow() %>%
add_recipe(recipe_q4) %>%
add_model(qda_spec)
# Store the results of fit() in a new variable
fit_workflow_q7 <- fit(workflow_q7, data = train_data)
# Display the results to check the model fit
fit_workflow_q7
# Specify a k-NN model with the 'kknn' engine and choose k (e.g., k = 8)
knn_spec <- nearest_neighbor(neighbors = 8) %>%
set_engine("kknn") %>%
set_mode("classification")
# Create a new workflow combining recipe_q4 and the new k nearest neghbors model
workflow_q8 <- workflow() %>%
add_recipe(recipe_q4) %>%
add_model(knn_spec)
# Store the results of fit() in a new variable
fit_workflow_q8 <- fit(workflow_q8, data = train_data)
# Display the results to check the model fit
fit_workflow_q8
# Generate predictions for each of the 4 models using the training data
pred_logistic <- predict(fit_workflow_q5, new_data = train_data, type = "prob")%>%
as_tibble()
pred_lda <- predict(fit_workflow_q6, new_data = train_data, type = "prob") %>%
as_tibble()
pred_qda <- predict(fit_workflow_q7, new_data = train_data, type = "prob") %>%
as_tibble()
pred_knn <- predict(fit_workflow_q8, new_data = train_data, type = "prob") %>%
as_tibble()
# Bind the predictions into one tibble
all_predictions <- bind_cols(
"Logistic" = pred_logistic$.pred_Yes,
"LDA" = pred_lda$.pred_Yes,
"QDA" = pred_qda$.pred_Yes,
"k-NN" = pred_knn$.pred_Yes
)
# Create data frames to store truth and predicted probabilities
df_logistic <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_logistic$.pred_Yes)
df_lda <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_lda$.pred_Yes)
df_qda <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_qda$.pred_Yes)
df_knn <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_knn$.pred_Yes)
# Calculate AUC-ROC for each model
roc_logistic <- roc_auc(df_logistic, truth, .pred_Yes)
roc_lda <- roc_auc(df_lda, truth, .pred_Yes)
roc_qda <- roc_auc(df_qda, truth, .pred_Yes)
roc_knn <- roc_auc(df_knn, truth, .pred_Yes)
# Display AUC-ROC for each model
print(paste("AUC-ROC for Logistic Regression: ", roc_logistic$.estimate))
print(paste("AUC-ROC for LDA: ", roc_lda$.estimate))
print(paste("AUC-ROC for QDA: ", roc_qda$.estimate))
print(paste("AUC-ROC for k-NN: ", roc_knn$.estimate))
all_predictions
pred_logistic
View(pred_logistic)
roc_logistic <- roc_auc(df_logistic, false, .pred_Yes)
View(df_logistic)
View(train_data)
# Generate predictions for each of the 4 models using the training data
pred_logistic <- predict(fit_workflow_q5, new_data = train_data, type = "prob")%>%
as_tibble()
pred_lda <- predict(fit_workflow_q6, new_data = train_data, type = "prob") %>%
as_tibble()
pred_qda <- predict(fit_workflow_q7, new_data = train_data, type = "prob") %>%
as_tibble()
pred_knn <- predict(fit_workflow_q8, new_data = train_data, type = "prob") %>%
as_tibble()
# Bind the predictions into one tibble
all_predictions <- bind_cols(
"Logistic" = pred_logistic$.pred_Yes,
"LDA" = pred_lda$.pred_Yes,
"QDA" = pred_qda$.pred_Yes,
"k-NN" = pred_knn$.pred_Yes
)
# Create data frames to store truth and predicted probabilities
df_logistic <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_logistic$.pred_Yes)
df_lda <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_lda$.pred_Yes)
df_qda <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_qda$.pred_Yes)
df_knn <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_knn$.pred_Yes)
# Calculate AUC-ROC for each model
roc_logistic <- roc_auc(df_logistic, truth, .pred_Yes)
roc_lda <- roc_auc(df_lda, truth, .pred_Yes)
roc_qda <- roc_auc(df_qda, truth, .pred_Yes)
roc_knn <- roc_auc(df_knn, truth, .pred_Yes)
# Display AUC-ROC for each model
print(paste("AUC-ROC for Logistic Regression: ", roc_logistic$.estimate))
print(paste("AUC-ROC for LDA: ", roc_lda$.estimate))
print(paste("AUC-ROC for QDA: ", roc_qda$.estimate))
print(paste("AUC-ROC for k-NN: ", roc_knn$.estimate))
df_logistic$truth <- as.numeric(df_logistic$truth == "Yes") - 1
# Generate predictions for each of the 4 models using the training data
pred_logistic <- predict(fit_workflow_q5, new_data = train_data, type = "prob")%>%
as_tibble()
pred_lda <- predict(fit_workflow_q6, new_data = train_data, type = "prob") %>%
as_tibble()
pred_qda <- predict(fit_workflow_q7, new_data = train_data, type = "prob") %>%
as_tibble()
pred_knn <- predict(fit_workflow_q8, new_data = train_data, type = "prob") %>%
as_tibble()
# Bind the predictions into one tibble
all_predictions <- bind_cols(
"Logistic" = pred_logistic$.pred_Yes,
"LDA" = pred_lda$.pred_Yes,
"QDA" = pred_qda$.pred_Yes,
"k-NN" = pred_knn$.pred_Yes
)
# Create data frames to store truth and predicted probabilities
df_logistic <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_logistic$.pred_Yes)
df_lda <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_lda$.pred_Yes)
df_qda <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_qda$.pred_Yes)
df_knn <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_knn$.pred_Yes)
# Calculate AUC-ROC for each model
roc_logistic <- roc_auc(df_logistic, truth, .pred_Yes)
roc_lda <- roc_auc(df_lda, truth, .pred_Yes)
roc_qda <- roc_auc(df_qda, truth, .pred_Yes)
roc_knn <- roc_auc(df_knn, truth, .pred_Yes)
# Display AUC-ROC for each model
print(paste("AUC-ROC for Logistic Regression: ", roc_logistic$.estimate))
print(paste("AUC-ROC for LDA: ", roc_lda$.estimate))
print(paste("AUC-ROC for QDA: ", roc_qda$.estimate))
print(paste("AUC-ROC for k-NN: ", roc_knn$.estimate))
# Generate predictions for each of the 4 models using the training data
pred_logistic <- predict(fit_workflow_q5, new_data = train_data, type = "prob")%>%
as_tibble()
pred_lda <- predict(fit_workflow_q6, new_data = train_data, type = "prob") %>%
as_tibble()
pred_qda <- predict(fit_workflow_q7, new_data = train_data, type = "prob") %>%
as_tibble()
pred_knn <- predict(fit_workflow_q8, new_data = train_data, type = "prob") %>%
as_tibble()
# Bind the predictions into one tibble
all_predictions <- bind_cols(
"Logistic" = pred_logistic$.pred_Yes,
"LDA" = pred_lda$.pred_Yes,
"QDA" = pred_qda$.pred_Yes,
"k-NN" = pred_knn$.pred_Yes
)
# Create data frames to store truth and predicted probabilities
df_logistic <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_logistic$.pred_Yes)
df_logistic$truth <- as.numeric(df_logistic$truth == "Yes") - 1
df_lda <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_lda$.pred_Yes)
df_qda <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_qda$.pred_Yes)
df_knn <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_knn$.pred_Yes)
# Calculate AUC-ROC for each model
roc_logistic <- roc_auc(df_logistic, truth, .pred_Yes)
# Recode the truth variable to 0 and 1, where 1 represents "Yes" (Survived)
df_logistic$truth <- as.numeric(df_logistic$truth == "Yes")
df_lda$truth <- as.numeric(df_lda$truth == "Yes")
df_qda$truth <- as.numeric(df_qda$truth == "Yes")
df_knn$truth <- as.numeric(df_knn$truth == "Yes")
# Generate predictions for each of the 4 models using the training data
pred_logistic <- predict(fit_workflow_q5, new_data = train_data, type = "prob")%>%
as_tibble()
pred_lda <- predict(fit_workflow_q6, new_data = train_data, type = "prob") %>%
as_tibble()
pred_qda <- predict(fit_workflow_q7, new_data = train_data, type = "prob") %>%
as_tibble()
pred_knn <- predict(fit_workflow_q8, new_data = train_data, type = "prob") %>%
as_tibble()
# Bind the predictions into one tibble
all_predictions <- bind_cols(
"Logistic" = pred_logistic$.pred_Yes,
"LDA" = pred_lda$.pred_Yes,
"QDA" = pred_qda$.pred_Yes,
"k-NN" = pred_knn$.pred_Yes
)
# Create data frames to store truth and predicted probabilities
df_logistic <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_logistic$.pred_Yes)
df_lda <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_lda$.pred_Yes)
df_qda <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_qda$.pred_Yes)
df_knn <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_knn$.pred_Yes)
# Recode the truth variable to 0 and 1, where 1 represents "Yes" (Survived)
df_logistic$truth <- as.numeric(df_logistic$truth == "Yes")
df_lda$truth <- as.numeric(df_lda$truth == "Yes")
df_qda$truth <- as.numeric(df_qda$truth == "Yes")
df_knn$truth <- as.numeric(df_knn$truth == "Yes")
# Calculate AUC-ROC for each model
roc_logistic <- roc_auc(df_logistic, truth, .pred_Yes)
# Generate predictions for each of the 4 models using the training data
pred_logistic <- predict(fit_workflow_q5, new_data = train_data, type = "prob")%>%
as_tibble()
pred_lda <- predict(fit_workflow_q6, new_data = train_data, type = "prob") %>%
as_tibble()
pred_qda <- predict(fit_workflow_q7, new_data = train_data, type = "prob") %>%
as_tibble()
pred_knn <- predict(fit_workflow_q8, new_data = train_data, type = "prob") %>%
as_tibble()
# Bind the predictions into one tibble
all_predictions <- bind_cols(
"Logistic" = pred_logistic$.pred_Yes,
"LDA" = pred_lda$.pred_Yes,
"QDA" = pred_qda$.pred_Yes,
"k-NN" = pred_knn$.pred_Yes
)
# Create data frames to store truth and predicted probabilities
df_logistic <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_logistic$.pred_Yes)
df_lda <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_lda$.pred_Yes)
df_qda <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_qda$.pred_Yes)
df_knn <- tibble(truth = as.factor(train_data$survived), .pred_Yes =
pred_knn$.pred_Yes)
# Recode the truth variable back to a factor, where "Yes" is the reference level
df_logistic$truth <- as.factor(ifelse(df_logistic$truth == 1, "Yes", "No"))
df_lda$truth <- as.factor(ifelse(df_lda$truth == 1, "Yes", "No"))
df_qda$truth <- as.factor(ifelse(df_qda$truth == 1, "Yes", "No"))
df_knn$truth <- as.factor(ifelse(df_knn$truth == 1, "Yes", "No"))
# Reorder the factor levels so that "Yes" is the reference level
df_logistic$truth <- relevel(df_logistic$truth, ref = "Yes")
# Check the levels in the 'truth' column of df_logistic
levels(df_logistic$truth)
table(train_data$survived)
# Create data frames to store truth and predicted probabilities
df_logistic <- tibble(truth = as.factor(train_data$survived), .pred_Yes = pred_logistic$.pred_Yes)
df_logistic
View(df_logistic)
nrow(pred_logistic)
nrow(train_data)
df_logistic <- tibble(truth = as.factor(train_data$survived), .pred_Yes = pred_logistic$.pred_Yes)
levels(df_logistic$truth)
# Reorder the factor levels so that "Yes" is the reference level
df_logistic$truth <- relevel(df_logistic$truth, ref = "Yes")
# Calculate AUC-ROC for Logistic Regression
roc_logistic <- roc_auc(df_logistic, truth, .pred_Yes)
# Display AUC-ROC for Logistic Regression
print(paste("AUC-ROC for Logistic Regression: ", roc_logistic$.estimate))
roc_logistic
View(roc_logistic)
# Generate predictions for each of the 4 models using the training data
pred_logistic <- predict(fit_workflow_q5, new_data = train_data, type = "prob") %>% as_tibble()
pred_lda <- predict(fit_workflow_q6, new_data = train_data, type = "prob") %>% as_tibble()
pred_qda <- predict(fit_workflow_q7, new_data = train_data, type = "prob") %>% as_tibble()
pred_knn <- predict(fit_workflow_q8, new_data = train_data, type = "prob") %>% as_tibble()
# Create data frames to store truth and predicted probabilities
df_logistic <- tibble(truth = as.factor(train_data$survived), .pred_Yes = pred_logistic$.pred_Yes)
df_lda <- tibble(truth = as.factor(train_data$survived), .pred_Yes = pred_lda$.pred_Yes)
df_qda <- tibble(truth = as.factor(train_data$survived), .pred_Yes = pred_qda$.pred_Yes)
df_knn <- tibble(truth = as.factor(train_data$survived), .pred_Yes = pred_knn$.pred_Yes)
# Reorder the factor levels so that "Yes" is the reference level for each data frame
df_logistic$truth <- relevel(df_logistic$truth, ref = "Yes")
df_lda$truth <- relevel(df_lda$truth, ref = "Yes")
df_qda$truth <- relevel(df_qda$truth, ref = "Yes")
df_knn$truth <- relevel(df_knn$truth, ref = "Yes")
# Calculate AUC-ROC for each model
roc_logistic <- roc_auc(df_logistic, truth, .pred_Yes)
roc_lda <- roc_auc(df_lda, truth, .pred_Yes)
roc_qda <- roc_auc(df_qda, truth, .pred_Yes)
roc_knn <- roc_auc(df_knn, truth, .pred_Yes)
# Display AUC-ROC for each model
print(paste("AUC-ROC for Logistic Regression: ", roc_logistic$.estimate))
print(paste("AUC-ROC for LDA: ", roc_lda$.estimate))
print(paste("AUC-ROC for QDA: ", roc_qda$.estimate))
print(paste("AUC-ROC for k-NN: ", roc_knn$.estimate))
# Generate predictions for each of the 4 models using the testing data
pred_logistic_test <- predict(fit_workflow_q5, new_data = test_data,
type = "prob") %>% as_tibble()
pred_lda_test <- predict(fit_workflow_q6, new_data = test_data,
type = "prob") %>% as_tibble()
pred_qda_test <- predict(fit_workflow_q7, new_data = test_data,
type = "prob") %>% as_tibble()
pred_knn_test <- predict(fit_workflow_q8, new_data = test_data,
type = "prob") %>% as_tibble()
# Create data frames to store truth and predicted probabilities for testing data
df_logistic_test <- tibble(truth = as.factor(test_data$survived),
.pred_Yes = pred_logistic_test$.pred_Yes)
df_lda_test <- tibble(truth = as.factor(test_data$survived),
.pred_Yes = pred_lda_test$.pred_Yes)
df_qda_test <- tibble(truth = as.factor(test_data$survived),
.pred_Yes = pred_qda_test$.pred_Yes)
df_knn_test <- tibble(truth = as.factor(test_data$survived),
.pred_Yes = pred_knn_test$.pred_Yes)
# Reorder the factor levels so that "Yes" is the reference level for each data frame
df_logistic_test$truth <- relevel(df_logistic_test$truth, ref = "Yes")
df_lda_test$truth <- relevel(df_lda_test$truth, ref = "Yes")
df_qda_test$truth <- relevel(df_qda_test$truth, ref = "Yes")
df_knn_test$truth <- relevel(df_knn_test$truth, ref = "Yes")
# Calculate AUC-ROC for each model on testing data
roc_logistic_test <- roc_auc(df_logistic_test, truth, .pred_Yes)
roc_lda_test <- roc_auc(df_lda_test, truth, .pred_Yes)
roc_qda_test <- roc_auc(df_qda_test, truth, .pred_Yes)
roc_knn_test <- roc_auc(df_knn_test, truth, .pred_Yes)
# Display AUC-ROC for each model on testing data
print(paste("Test AUC-ROC for Logistic Regression: ",
roc_logistic_test$.estimate))
print(paste("Test AUC-ROC for LDA: ",
roc_lda_test$.estimate))
print(paste("Test AUC-ROC for QDA: ",
roc_qda_test$.estimate))
print(paste("Test AUC-ROC for k-NN: ",
roc_knn_test$.estimate))
## Based on the output above, Logistic Regression model seems has the highest AUC-ROC performance.
# Generate class predictions for the best-performing model (Logistic Regression)
pred_class_logistic_test <- predict(fit_workflow_q5, new_data = test_data, type = "class") %>% as_tibble()
# Create the confusion matrix
confusion <- conf_mat(test_data, truth = survived, estimate = pred_class_logistic_test$.pred_class)
# Check the names of the columns
names(pred_class_logistic_test)
# Check if 'survived' exists in test_data
names(test_data)
## Based on the output above, Logistic Regression model seems has the highest AUC-ROC performance.
# Create a new tibble combining 'survived' and '.pred_class'
combined_data <- tibble(
truth = test_data$survived,
.pred_class = pred_class_logistic_test$.pred_class
)
# Create the confusion matrix
confusion <- conf_mat(combined_data, truth = truth, estimate = .pred_class)
# Print the confusion matrix
print(confusion)
# Visualize the confusion matrix using ggplot2
autoplot(confusion)
# Create a new tibble combining 'survived' and predicted probabilities
roc_data <- tibble(
truth = as.factor(test_data$survived),
.pred_Yes = pred_prob_logistic_test$.pred_Yes
)
## Based on the output above, Logistic Regression model seems has the highest AUC-ROC performance.
# Create a new tibble combining 'survived' and '.pred_class'
combined_data <- tibble(
truth = test_data$survived,
.pred_class = pred_class_logistic_test$.pred_class
)
# Create the confusion matrix
confusion <- conf_mat(combined_data, truth = truth, estimate = .pred_class)
# Print the confusion matrix
print(confusion)
# Visualize the confusion matrix using ggplot2
autoplot(confusion)
# Create a new tibble combining 'survived' and predicted probabilities
roc_data <- tibble(
truth = as.factor(test_data$survived),
.pred_Yes = pred_logistic_test$.pred_Yes
)
# Create the ROC curve
roc_curve_data <- roc_curve(roc_data, truth, .pred_Yes)
# Plot the ROC curve using ggplot2
ggplot(roc_curve_data, aes(x = 1 - specificity, y = sensitivity)) +
geom_line() +
geom_abline(linetype = "dashed") +
labs(title = "ROC Curve for Logistic Regression",
x = "1 - Specificity",
y = "Sensitivity")
## Based on the output above, Logistic Regression model seems has the highest AUC-ROC performance.
# Create a new tibble combining 'survived' and '.pred_class'
combined_data <- tibble(
truth = test_data$survived,
.pred_class = pred_class_logistic_test$.pred_class
)
# Create the confusion matrix
confusion <- conf_mat(combined_data, truth = truth, estimate = .pred_class)
# Print the confusion matrix
print(confusion)
# Visualize the confusion matrix using ggplot2
autoplot(confusion)
# Create a new tibble combining 'survived' and predicted probabilities
roc_data <- tibble(
truth = as.factor(test_data$survived),
.pred_Yes = pred_logistic_test$.pred_Yes
)
# Create the ROC curve
roc_curve_data <- roc_curve(roc_data, truth, .pred_Yes)
# Plot the ROC curve using ggplot2
ggplot(roc_curve_data, aes(x =  specificity, y = sensitivity)) +
geom_line() +
geom_abline(linetype = "dashed") +
labs(title = "ROC Curve for Logistic Regression",
x = "Specificity",
y = "Sensitivity")
## Based on the output above, Logistic Regression model seems has the highest AUC-ROC performance.
# Create a new tibble combining 'survived' and '.pred_class'
combined_data <- tibble(
truth = test_data$survived,
.pred_class = pred_logistic_test$.pred_class
)
# Create the confusion matrix
confusion <- conf_mat(combined_data, truth = truth, estimate = .pred_class)
## Based on the output above, Logistic Regression model seems has the highest AUC-ROC performance.
# Create a new tibble combining 'survived' and '.pred_class'
combined_data <- tibble(
truth = test_data$survived,
.pred_class = pred_logistic_test$.pred_class
)
# Create the confusion matrix
confusion <- conf_mat(combined_data, truth = truth, estimate = .pred_class)
## Based on the output above, Logistic Regression model seems has the highest AUC-ROC performance.
# Create a new tibble combining 'survived' and '.pred_class'
combined_data <- tibble(
truth = test_data$survived,
.pred_class = pred_class_logistic_test$.pred_class
)
# Create the confusion matrix
confusion <- conf_mat(combined_data, truth = truth, estimate = .pred_class)
# Print the confusion matrix
print(confusion)
# Visualize the confusion matrix using ggplot2
autoplot(confusion)
# Create a new tibble combining 'survived' and predicted probabilities
roc_data <- tibble(
truth = as.factor(test_data$survived),
.pred_Yes = pred_logistic_test$.pred_Yes
)
# Create the ROC curve
roc_curve_data <- roc_curve(roc_data, truth, .pred_Yes)
# Plot the ROC curve using ggplot2
ggplot(roc_curve_data, aes(x =  specificity, y = sensitivity)) +
geom_line() +
geom_abline(linetype = "dashed") +
labs(title = "ROC Curve for Logistic Regression",
x = "Specificity",
y = "Sensitivity")
## Based on the output above, Logistic Regression model seems has the highest AUC-ROC performance.
# Create a new tibble combining 'survived' and '.pred_class'
combined_data <- tibble(
truth = test_data$survived,
.pred_class = pred_logistic_test$.pred_class
)
# Create the confusion matrix
confusion <- conf_mat(combined_data, truth = truth, estimate = .pred_class)
combined_data
View(combined_data)
View(pred_logistic_test)
View(pred_logistic_test)
## Based on the output above, Logistic Regression model seems has the highest AUC-ROC performance.
# Create a new column in pred_logistic_test for predicted class labels
pred_logistic_test$.pred_class <- ifelse(pred_logistic_test$.pred_Yes > 0.5,
"Yes", "No")
# Create a new tibble combining 'survived' and '.pred_class'
combined_data <- tibble(
truth = test_data$survived,
.pred_class = pred_logistic_test$.pred_class
)
# Create the confusion matrix
confusion <- conf_mat(combined_data, truth = truth, estimate = .pred_class)
## Based on the output above, Logistic Regression model seems has the highest AUC-ROC performance.
# Create a new column in pred_logistic_test for predicted class labels
pred_logistic_test$.pred_class <- ifelse(pred_logistic_test$.pred_Yes > 0.5,
"Yes", "No")
# Create a new tibble combining 'survived' and '.pred_class'
combined_data <- tibble(
truth = test_data$survived,
.pred_class = pred_logistic_test$.pred_class
)
# Convert .pred_class to a factor
combined_data$.pred_class <- as.factor(combined_data$.pred_class)
# Create the confusion matrix
confusion <- conf_mat(combined_data, truth = truth, estimate = .pred_class)
# Visualize the confusion matrix
autoplot(confusion)
# Create a new tibble combining 'survived' and predicted probabilities
roc_data <- tibble(
truth = as.factor(test_data$survived),
.pred_Yes = pred_logistic_test$.pred_Yes
)
# Create the ROC curve
roc_curve_data <- roc_curve(roc_data, truth, .pred_Yes)
# Plot the ROC curve using ggplot2
ggplot(roc_curve_data, aes(x =  specificity, y = sensitivity)) +
geom_line() +
geom_abline(linetype = "dashed") +
labs(title = "ROC Curve for Logistic Regression",
x = "Specificity",
y = "Sensitivity")
