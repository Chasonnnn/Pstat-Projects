enet_grid <- grid_regular(
penalty(),
mixture(range = c(0, 1)),
levels = 10
)
# Check all grids
print(knn_grid)
print(rf_grid)
print(enet_grid)
# Fit k-NN Model
knn_results <- tune_grid(
object = knn_wf,
resamples = folds,
grid = knn_grid
)
# Check notes/warnings
show_notes(knn_results)
# Save the knn_results object to a .rds file
saveRDS(knn_results, "knn_results.rds")
# Fit Linear Regression Model
lm_results <- tune_grid(
object = lm_wf,
resamples = folds
)
# Check notes/warnings
show_notes(lm_results)
# Save the lm_results object to a .rds file
saveRDS(lm_results, "lm_results.rds")
best_knn <- knn_results %>% show_best("rmse", n = 1)
best_lm <- lm_results %>% show_best("rmse", n = 1)
print(best_knn)
print(best_lm)
# k-NN Model Specification
knn_spec <- nearest_neighbor(
neighbors = tune(),
engine = "kknn") %>%
set_mode("regression")
# k-NN Workflow
knn_wf <- workflow() %>%
add_model(knn_spec) %>%
add_recipe(real_estate_recipe)
# Linear Regression Model Specification
lm_spec <- linear_reg() %>%
set_engine("lm") %>%
set_mode("regression")
# Linear Regression Workflow
lm_wf <- workflow() %>%
add_model(lm_spec) %>%
add_recipe(real_estate_recipe)
# Random Forest Model Specification
rf_spec <- rand_forest(
mtry = tune(),
trees = tune(),
min_n = tune(),
mode = "regression") %>%
set_engine("ranger", importance = "impurity")
# Random Forest Workflow
rf_wf <- workflow() %>%
add_model(rf_spec) %>%
add_recipe(real_estate_recipe)
# Elastic Net Model Specification
enet_spec <- linear_reg(
penalty = tune(),
mixture = tune()) %>%
set_engine("glmnet") %>%
set_mode("regression")
# Elastic Net Workflow
enet_wf <- workflow() %>%
add_model(enet_spec) %>%
add_recipe(real_estate_recipe)
# Fit Random Forest Model
rf_results <- tune_grid(
object = rf_wf,
resamples = folds,
grid = rf_grid
)
# Check notes/warnings
show_notes(rf_results)
# Save the rf_results object to a .rds file
saveRDS(rf_results, "rf_results.rds")
# Fit Elastic Net Model
enet_results <- tune_grid(
object = enet_wf,
resamples = folds,
grid = enet_grid
)
# Check notes/warnings
show_notes(enet_results)
# Save the enet_results object to a .rds file
saveRDS(enet_results, "enet_results.rds")
# Load the knn_results object from the .rds file
knn_results <- readRDS("knn_results.rds")
# Load the lm_results object from the .rds file
lm_results <- readRDS("lm_results.rds")
# Load the rf_results object from the .rds file
rf_results <- readRDS("rf_results.rds")
# Load the enet_results object from the .rds file
enet_results <- readRDS("enet_results.rds")
# Collect Metrics
knn_metrics <- collect_metrics(knn_results)
lm_metrics <- collect_metrics(lm_results)
rf_metrics <- collect_metrics(rf_results)
enet_metrics <- collect_metrics(enet_results)
# Print a summary of each model
summary(knn_metrics)
summary(lm_metrics)
summary(rf_metrics)
summary(enet_metrics)
# Identifying the Best Model of each type:
best_knn <- knn_results %>% show_best("rmse", n = 1)
best_lm <- lm_results %>% show_best("rmse", n = 1)
best_rf <- rf_results %>% show_best("rmse", n = 1)
best_enet <- enet_results %>% show_best("rmse", n = 1)
# Print the best models.
print(best_knn)
print(best_lm)
print(best_rf)
print(best_enet)
# Set the seed to ensure reproducibility.
set.seed(123)
# Create a smaller dataset containing 4000 observations.
small_data <- sample_n(data_cleaned, size = 4000) %>% select(-price_per_area, -bed_bath_ratio)
# 70% of the data will go into the training set, and 30% will go into the test set.
# The split is stratified by price.
data_split <- initial_split(small_data, prop = 0.7, strata = "price")
train_data <- training(data_split)
test_data <- testing(data_split)
#Check the First Few Rows of Both Sets:
head(train_data)
head(test_data)
# Create 5 Folds from the Training Set for Cross-Validation.
folds <- vfold_cv(train_data, v = 5, strata = "price")
# Create the recipe
real_estate_recipe <- recipe(price ~ .,
data = train_data) %>%
step_novel(all_nominal_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_mutate(interaction_1 = livingArea * bathrooms,
interaction_2 = bedrooms * bathrooms) %>%
step_zv(all_predictors()) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors())
# Print the recipe to check it
print(real_estate_recipe)
# k-NN Model Specification
knn_spec <- nearest_neighbor(
neighbors = tune(),
engine = "kknn") %>%
set_mode("regression")
# k-NN Workflow
knn_wf <- workflow() %>%
add_model(knn_spec) %>%
add_recipe(real_estate_recipe)
# Linear Regression Model Specification
lm_spec <- linear_reg() %>%
set_engine("lm") %>%
set_mode("regression")
# Linear Regression Workflow
lm_wf <- workflow() %>%
add_model(lm_spec) %>%
add_recipe(real_estate_recipe)
# Random Forest Model Specification
rf_spec <- rand_forest(
mtry = tune(),
trees = tune(),
min_n = tune(),
mode = "regression") %>%
set_engine("ranger", importance = "impurity")
# Random Forest Workflow
rf_wf <- workflow() %>%
add_model(rf_spec) %>%
add_recipe(real_estate_recipe)
# Elastic Net Model Specification
enet_spec <- linear_reg(
penalty = tune(),
mixture = tune()) %>%
set_engine("glmnet") %>%
set_mode("regression")
# Elastic Net Workflow
enet_wf <- workflow() %>%
add_model(enet_spec) %>%
add_recipe(real_estate_recipe)
# k-NN Grid: Trying neighbors from 1 to 10, in 6 levels
knn_grid <- grid_regular(
neighbors(range = c(1, 10)),
levels = 6
)
# Random Forest Grid
rf_grid <- grid_regular(
mtry(range = c(1, 7)),
trees(range = c(50, 500)),
min_n(range = c(2, 20)),
levels = 8
)
# Elastic Net Grid
enet_grid <- grid_regular(
penalty(),
mixture(range = c(0, 1)),
levels = 10
)
# Check all grids
print(knn_grid)
print(rf_grid)
print(enet_grid)
# Fit k-NN Model
knn_results <- tune_grid(
object = knn_wf,
resamples = folds,
grid = knn_grid
)
# Check notes/warnings
show_notes(knn_results)
# Save the knn_results object to a .rds file
saveRDS(knn_results, "knn_results.rds")
# Fit Linear Regression Model
lm_results <- tune_grid(
object = lm_wf,
resamples = folds
)
# Check notes/warnings
show_notes(lm_results)
# Save the lm_results object to a .rds file
saveRDS(lm_results, "lm_results.rds")
best_knn <- knn_results %>% show_best("rmse", n = 1)
best_lm <- lm_results %>% show_best("rmse", n = 1)
print(best_knn)
print(best_lm)
# Create the recipe
real_estate_recipe <- recipe(price ~ .,
data = train_data) %>%
step_novel(all_nominal_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors()) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors())
# Print the recipe to check it
print(real_estate_recipe)
# k-NN Model Specification
knn_spec <- nearest_neighbor(
neighbors = tune(),
engine = "kknn") %>%
set_mode("regression")
# k-NN Workflow
knn_wf <- workflow() %>%
add_model(knn_spec) %>%
add_recipe(real_estate_recipe)
# Linear Regression Model Specification
lm_spec <- linear_reg() %>%
set_engine("lm") %>%
set_mode("regression")
# Linear Regression Workflow
lm_wf <- workflow() %>%
add_model(lm_spec) %>%
add_recipe(real_estate_recipe)
# Random Forest Model Specification
rf_spec <- rand_forest(
mtry = tune(),
trees = tune(),
min_n = tune(),
mode = "regression") %>%
set_engine("ranger", importance = "impurity")
# Random Forest Workflow
rf_wf <- workflow() %>%
add_model(rf_spec) %>%
add_recipe(real_estate_recipe)
# Elastic Net Model Specification
enet_spec <- linear_reg(
penalty = tune(),
mixture = tune()) %>%
set_engine("glmnet") %>%
set_mode("regression")
# Elastic Net Workflow
enet_wf <- workflow() %>%
add_model(enet_spec) %>%
add_recipe(real_estate_recipe)
# k-NN Grid: Trying neighbors from 1 to 10, in 6 levels
knn_grid <- grid_regular(
neighbors(range = c(1, 10)),
levels = 6
)
# Random Forest Grid
rf_grid <- grid_regular(
mtry(range = c(1, 7)),
trees(range = c(50, 500)),
min_n(range = c(2, 20)),
levels = 8
)
# Elastic Net Grid
enet_grid <- grid_regular(
penalty(),
mixture(range = c(0, 1)),
levels = 10
)
# Check all grids
print(knn_grid)
print(rf_grid)
print(enet_grid)
# Fit k-NN Model
knn_results <- tune_grid(
object = knn_wf,
resamples = folds,
grid = knn_grid
)
# Check notes/warnings
show_notes(knn_results)
# Save the knn_results object to a .rds file
saveRDS(knn_results, "knn_results.rds")
# Fit Linear Regression Model
lm_results <- tune_grid(
object = lm_wf,
resamples = folds
)
# Check notes/warnings
show_notes(lm_results)
# Save the lm_results object to a .rds file
saveRDS(lm_results, "lm_results.rds")
best_knn <- knn_results %>% show_best("rmse", n = 1)
best_lm <- lm_results %>% show_best("rmse", n = 1)
print(best_knn)
print(best_lm)
# Set the seed to ensure reproducibility.
set.seed(123)
# Create a smaller dataset containing 4000 observations.
small_data <- sample_n(data_cleaned, size = 4000) %>% select(-price_per_area, -bed_bath_ratio)
# 70% of the data will go into the training set, and 30% will go into the test set.
# The split is stratified by price.
data_split <- initial_split(small_data, prop = 0.7, strata = "price")
train_data <- training(data_split)
test_data <- testing(data_split)
#Check the First Few Rows of Both Sets:
head(train_data)
head(test_data)
# Create 5 Folds from the Training Set for Cross-Validation.
folds <- vfold_cv(train_data, v = 5, strata = "price")
# Create the recipe
real_estate_recipe <- recipe(price ~ .,
data = train_data) %>%
step_novel(all_nominal_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors()) %>%
step_center(all_predictors()) %>%
step_scale(all_predictors())
# Print the recipe to check it
print(real_estate_recipe)
# k-NN Model Specification
knn_spec <- nearest_neighbor(
neighbors = tune(),
engine = "kknn") %>%
set_mode("regression")
# k-NN Workflow
knn_wf <- workflow() %>%
add_model(knn_spec) %>%
add_recipe(real_estate_recipe)
# Linear Regression Model Specification
lm_spec <- linear_reg() %>%
set_engine("lm") %>%
set_mode("regression")
# Linear Regression Workflow
lm_wf <- workflow() %>%
add_model(lm_spec) %>%
add_recipe(real_estate_recipe)
# Random Forest Model Specification
rf_spec <- rand_forest(
mtry = tune(),
trees = tune(),
min_n = tune(),
mode = "regression") %>%
set_engine("ranger", importance = "impurity")
# Random Forest Workflow
rf_wf <- workflow() %>%
add_model(rf_spec) %>%
add_recipe(real_estate_recipe)
# Elastic Net Model Specification
enet_spec <- linear_reg(
penalty = tune(),
mixture = tune()) %>%
set_engine("glmnet") %>%
set_mode("regression")
# Elastic Net Workflow
enet_wf <- workflow() %>%
add_model(enet_spec) %>%
add_recipe(real_estate_recipe)
# k-NN Grid: Trying neighbors from 1 to 10, in 6 levels
knn_grid <- grid_regular(
neighbors(range = c(1, 10)),
levels = 6
)
# Random Forest Grid
rf_grid <- grid_regular(
mtry(range = c(1, 7)),
trees(range = c(50, 500)),
min_n(range = c(2, 20)),
levels = 8
)
# Elastic Net Grid
enet_grid <- grid_regular(
penalty(),
mixture(range = c(0, 1)),
levels = 10
)
# Check all grids
print(knn_grid)
print(rf_grid)
print(enet_grid)
# Fit k-NN Model
knn_results <- tune_grid(
object = knn_wf,
resamples = folds,
grid = knn_grid
)
# Check notes/warnings
show_notes(knn_results)
# Save the knn_results object to a .rds file
saveRDS(knn_results, "knn_results.rds")
# Fit Linear Regression Model
lm_results <- tune_grid(
object = lm_wf,
resamples = folds
)
# Check notes/warnings
show_notes(lm_results)
# Save the lm_results object to a .rds file
saveRDS(lm_results, "lm_results.rds")
# Fit Random Forest Model
rf_results <- tune_grid(
object = rf_wf,
resamples = folds,
grid = rf_grid
)
# Check notes/warnings
show_notes(rf_results)
# Save the rf_results object to a .rds file
saveRDS(rf_results, "rf_results.rds")
# Fit Elastic Net Model
enet_results <- tune_grid(
object = enet_wf,
resamples = folds,
grid = enet_grid
)
# Check notes/warnings
show_notes(enet_results)
# Save the enet_results object to a .rds file
saveRDS(enet_results, "enet_results.rds")
# Load the knn_results object from the .rds file
knn_results <- readRDS("knn_results.rds")
# Load the lm_results object from the .rds file
lm_results <- readRDS("lm_results.rds")
# Load the rf_results object from the .rds file
rf_results <- readRDS("rf_results.rds")
# Load the enet_results object from the .rds file
enet_results <- readRDS("enet_results.rds")
# Collect Metrics
knn_metrics <- collect_metrics(knn_results)
lm_metrics <- collect_metrics(lm_results)
rf_metrics <- collect_metrics(rf_results)
enet_metrics <- collect_metrics(enet_results)
# Print a summary of each model
summary(knn_metrics)
summary(lm_metrics)
summary(rf_metrics)
summary(enet_metrics)
# Identifying the Best Model of each type:
best_knn <- knn_results %>% show_best("rmse", n = 1)
best_lm <- lm_results %>% show_best("rmse", n = 1)
best_rf <- rf_results %>% show_best("rmse", n = 1)
best_enet <- enet_results %>% show_best("rmse", n = 1)
# Print the best models.
print(best_knn)
print(best_lm)
print(best_rf)
print(best_enet)
# Fit the optimal Random Forest model to the training set
best_rf_model <- rand_forest(
mtry = 10,
trees = 500,
min_n = 7,
mode = "regression") %>%
set_engine("ranger", importance = "impurity") %>%
fit(price ~ ., data = train_data)
# Create the variable importance plot
vip(best_rf_model)
# Fit the optimal Random Forest model to the training set
best_rf_model <- rand_forest(
mtry = 4,
trees = 50,
min_n = 9,
mode = "regression") %>%
set_engine("ranger", importance = "impurity") %>%
fit(price ~ ., data = train_data)
# Create the variable importance plot
vip(best_rf_model)
# Augment the model to get the predictions in test_data
best_rf_model_test <- augment(best_rf_model, test_data) %>%
# Augment the model to get the predictions in test_data
best_rf_model_test <- augment(best_rf_model, test_data)
# Augment the model to get the predictions in test_data
best_rf_model_test <- augment(best_rf_model, test_data) %>%
select(price, starts_with(".pred"))
# Calculate RMSE
rmse_test <- sqrt(mean((best_rf_model_test$price - best_rf_model_test$.pred)^2))
print(rmse_test)
# Augment the model to get the predictions in test_data
best_rf_model_test <- augment(best_rf_model, test_data) %>%
select(price, starts_with(".pred"))
# Calculate RMSE
rmse_test <- sqrt(mean((best_rf_model_test$price - best_rf_model_test$.pred)^2))
# Print the RMSE calculated above
print(rmse_test)
save.image("~/Desktop/pstat131/data-memo/final project.RData")
data_cleaned
View(data_cleaned)
View(small_data)
